{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# PEFT Fine-tuning Project: GPT-2 with LoRA for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d24873d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3c5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f200dd",
   "metadata": {},
   "source": [
    "## 1. PREPARE THE FOUNDATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91596341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: Tesla T4\n",
      "Memory: 14917 MB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Memory:\", torch.cuda.get_device_properties(0).total_memory // (1024 ** 2), \"MB\")\n",
    "else:\n",
    "    print(\"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f743f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca5b0f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867289a398d441af820d079bf028259d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f603006809849cc9d606161798d5b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e62d36a0cbb4321b368881a4a403bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55ad3c6cfda4c15bfad90a0195619d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450deab4747e43149318d6321461e492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69678d053fe6491185fb4bebdae085a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a817cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model parameters: 124,441,344\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11edd2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5de29f608d34b5f8763bcf88873f8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f76bbc7517c4af79efa147b7cd4380c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "# Load full splits\n",
    "full_train = load_dataset(\"imdb\", split=\"train\")\n",
    "full_test = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "# Select 2500 positive and 2500 negative for train\n",
    "neg_train = full_train.filter(lambda x: x[\"label\"] == 0).select(range(2500))\n",
    "pos_train = full_train.filter(lambda x: x[\"label\"] == 1).select(range(2500))\n",
    "dataset = concatenate_datasets([neg_train, pos_train])\n",
    "\n",
    "# Select 500 positive and 500 negative for test\n",
    "neg_test = full_test.filter(lambda x: x[\"label\"] == 0).select(range(500))\n",
    "pos_test = full_test.filter(lambda x: x[\"label\"] == 1).select(range(500))\n",
    "test_dataset = concatenate_datasets([neg_test, pos_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c05abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=256\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f649434e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05cfff2a1bd4117ab684b90de0aaf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc446313b2cb4f91a708fd3cad8c21d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize datasets\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98139a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set format for PyTorch\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c948bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "389cb9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078dc8b8",
   "metadata": {},
   "source": [
    "## 2. EVALUATE PRETRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfb4537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments for evaluation\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./eval_results\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c74eb555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Original Model ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model - Accuracy: 0.5010, F1: 0.3442\n"
     ]
    }
   ],
   "source": [
    "# Create trainer for original model\n",
    "original_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Evaluating Original Model ===\")\n",
    "original_results = original_trainer.evaluate()\n",
    "print(f\"Original Model - Accuracy: {original_results['eval_accuracy']:.4f}, F1: {original_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab610e",
   "metadata": {},
   "source": [
    "## 3. PERFORM LIGHTWEIGHT FINE-TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab723581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 specific modules\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "258d9a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,625,088 || all params: 126,064,896 || trainable%: 1.2890884390211212\n"
     ]
    }
   ],
   "source": [
    "# Create PEFT model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9344195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    report_to=None,\n",
    "    learning_rate=2e-4,  # Higher LR for LoRA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55095ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training PEFT Model ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 12:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.498900</td>\n",
       "      <td>0.368763</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.883429</td>\n",
       "      <td>0.891677</td>\n",
       "      <td>0.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.467900</td>\n",
       "      <td>0.347817</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.914007</td>\n",
       "      <td>0.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.277900</td>\n",
       "      <td>0.419246</td>\n",
       "      <td>0.917000</td>\n",
       "      <td>0.916990</td>\n",
       "      <td>0.917202</td>\n",
       "      <td>0.917000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=0.4815634696960449, metrics={'train_runtime': 741.9001, 'train_samples_per_second': 20.218, 'train_steps_per_second': 5.055, 'total_flos': 1997132267520000.0, 'train_loss': 0.4815634696960449, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create trainer for PEFT model\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Training PEFT Model ===\")\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "930f2c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2_lora_imdb/tokenizer_config.json',\n",
       " './gpt2_lora_imdb/special_tokens_map.json',\n",
       " './gpt2_lora_imdb/vocab.json',\n",
       " './gpt2_lora_imdb/merges.txt',\n",
       " './gpt2_lora_imdb/added_tokens.json',\n",
       " './gpt2_lora_imdb/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the PEFT model\n",
    "peft_model.save_pretrained(\"./gpt2_lora_imdb\")\n",
    "tokenizer.save_pretrained(\"./gpt2_lora_imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048250d5",
   "metadata": {},
   "source": [
    "## 4. PERFORM INFERENCE USING FINE-TUNED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd1f50dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Saved PEFT Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Loading Saved PEFT Model ===\")\n",
    "\n",
    "# Load the saved PEFT model\n",
    "loaded_peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\"./gpt2_lora_imdb\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./gpt2_lora_imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5de81b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Linear(\n",
       "                in_features=768, out_features=2304, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_peft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f15dcf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if loaded_tokenizer.pad_token is None:\n",
    "    loaded_tokenizer.pad_token = loaded_tokenizer.eos_token  # or a suitable token\n",
    "    loaded_tokenizer.pad_token_id = loaded_tokenizer.convert_tokens_to_ids(loaded_tokenizer.pad_token)\n",
    "\n",
    "# Also set it in the model config\n",
    "loaded_peft_model.config.pad_token_id = loaded_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a80d5a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Fine-tuned Model ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create trainer for loaded PEFT model\n",
    "loaded_trainer = Trainer(\n",
    "    model=loaded_peft_model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=loaded_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Evaluating Fine-tuned Model ===\")\n",
    "finetuned_results = loaded_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26bbf902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PERFORMANCE COMPARISON\n",
      "==================================================\n",
      "Original Model:\n",
      "  Accuracy: 0.5010\n",
      "  F1 Score: 0.3442\n",
      "  Precision: 0.5230\n",
      "  Recall: 0.5010\n",
      "\n",
      "Fine-tuned Model:\n",
      "  Accuracy: 0.9170\n",
      "  F1 Score: 0.9170\n",
      "  Precision: 0.9172\n",
      "  Recall: 0.9170\n",
      "\n",
      "Improvement:\n",
      "  Accuracy: +0.4160\n",
      "  F1 Score: +0.5728\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original Model:\")\n",
    "print(f\"  Accuracy: {original_results['eval_accuracy']:.4f}\")\n",
    "print(f\"  F1 Score: {original_results['eval_f1']:.4f}\")\n",
    "print(f\"  Precision: {original_results['eval_precision']:.4f}\")\n",
    "print(f\"  Recall: {original_results['eval_recall']:.4f}\")\n",
    "\n",
    "print(f\"\\nFine-tuned Model:\")\n",
    "print(f\"  Accuracy: {finetuned_results['eval_accuracy']:.4f}\")\n",
    "print(f\"  F1 Score: {finetuned_results['eval_f1']:.4f}\")\n",
    "print(f\"  Precision: {finetuned_results['eval_precision']:.4f}\")\n",
    "print(f\"  Recall: {finetuned_results['eval_recall']:.4f}\")\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "accuracy_diff = finetuned_results['eval_accuracy'] - original_results['eval_accuracy']\n",
    "f1_diff = finetuned_results['eval_f1'] - original_results['eval_f1']\n",
    "print(f\"  Accuracy: {accuracy_diff:+.4f}\")\n",
    "print(f\"  F1 Score: {f1_diff:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88061a33",
   "metadata": {},
   "source": [
    "## EXAMPLE INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c087729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text 1: This movie was absolutely Great! Great acting and ...\n",
      "Prediction: Positive (1.000)\n",
      "\n",
      "Text 2: Terrible film, waste of time. Poor acting and bori...\n",
      "Prediction: Negative (1.000)\n",
      "\n",
      "Text 3: I loved the cinematography and the soundtrack was ...\n",
      "Prediction: Positive (1.000)\n",
      "\n",
      "Text 4: The plot was predictable and the characters were d...\n",
      "Prediction: Negative (0.999)\n",
      "\n",
      "Text 5: It was okay, not the best but not the worst either...\n",
      "Prediction: Negative (0.738)\n",
      "\n",
      "Text 6: What a masterpiece! I was on the edge of my seat t...\n",
      "Prediction: Positive (0.999)\n",
      "\n",
      "Text 7: Completely unwatchable. I walked out of the theate...\n",
      "Prediction: Negative (0.998)\n",
      "\n",
      "Text 8: Decent movie for a rainy afternoon. Nothing too sp...\n",
      "Prediction: Negative (0.592)\n",
      "\n",
      "Text 9: Absolutely stunning visuals and a compelling story...\n",
      "Prediction: Positive (1.000)\n",
      "\n",
      "Text 10: The jokes were flat and the pacing was off through...\n",
      "Prediction: Negative (0.999)\n",
      "\n",
      "Text 11: The performances were strong but the plot lacked o...\n",
      "Prediction: Negative (0.996)\n",
      "\n",
      "Text 12: Heartwarming and inspiring — definitely a must-wat...\n",
      "Prediction: Positive (1.000)\n",
      "\n",
      "=== Project Complete ===\n",
      "Saved files in './gpt2_lora_imdb':\n",
      "  - adapter_model.bin\n",
      "  - adapter_config.json\n",
      "  - README.md\n",
      "  - merges.txt\n",
      "  - tokenizer.json\n",
      "  - tokenizer_config.json\n",
      "  - special_tokens_map.json\n",
      "  - vocab.json\n"
     ]
    }
   ],
   "source": [
    "# Test examples\n",
    "test_texts = [\n",
    "    \"This movie was absolutely Great! Great acting and storyline.\",  # Positive\n",
    "    \"Terrible film, waste of time. Poor acting and boring plot.\",     # Negative\n",
    "    \"I loved the cinematography and the soundtrack was beautiful.\",   # Positive\n",
    "    \"The plot was predictable and the characters were dull.\",         # Negative\n",
    "    \"It was okay, not the best but not the worst either.\",            # Neutral\n",
    "    \"What a masterpiece! I was on the edge of my seat the entire time.\",  # Positive\n",
    "    \"Completely unwatchable. I walked out of the theater halfway through.\",  # Negative\n",
    "    \"Decent movie for a rainy afternoon. Nothing too special though.\",  # Neutral\n",
    "    \"Absolutely stunning visuals and a compelling story!\",            # Positive\n",
    "    \"The jokes were flat and the pacing was off throughout.\",         # Negative\n",
    "    \"The performances were strong but the plot lacked originality.\",  # Neutral\n",
    "    \"Heartwarming and inspiring — definitely a must-watch!\",          # Positive\n",
    "]\n",
    "\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_peft_model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "    \n",
    "    sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "    confidence = predictions[0][predicted_class].item()\n",
    "    \n",
    "    print(f\"\\nText {i+1}: {text[:50]}...\")\n",
    "    print(f\"Prediction: {sentiment} ({confidence:.3f})\")\n",
    "\n",
    "print(\"\\n=== Project Complete ===\")\n",
    "print(f\"Saved files in './gpt2_lora_imdb':\")\n",
    "for file in os.listdir(\"./gpt2_lora_imdb\"):\n",
    "    print(f\"  - {file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
